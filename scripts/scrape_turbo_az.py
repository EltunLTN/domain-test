"""
Turbo.az Web Scraper
Bu script turbo.az saytÄ±ndan avtomobil mÉ™lumatlarÄ±nÄ± scrape edir vÉ™ CSV faylÄ±na yazÄ±r.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from datetime import datetime
import random

class TurboAzScraper:
    def __init__(self):
        self.base_url = "https://turbo.az/autos"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'az-AZ,az;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        self.cars_data = []

    def clean_price(self, price_text):
        """QiymÉ™ti tÉ™mizlÉ™yir vÉ™ rÉ™qÉ™mÉ™ Ã§evirir"""
        if not price_text:
            return None
        # "12 500 AZN" -> 12500
        cleaned = re.sub(r'[^\d]', '', price_text)
        return int(cleaned) if cleaned else None

    def clean_number(self, text):
        """RÉ™qÉ™mlÉ™ri tÉ™mizlÉ™yir"""
        if not text:
            return None
        cleaned = re.sub(r'[^\d.]', '', text)
        return float(cleaned) if cleaned else None

    def scrape_page(self, page_num):
        """Bir sÉ™hifÉ™ni scrape edir"""
        print(f"Scraping sÉ™hifÉ™ {page_num}...")
        
        url = f"{self.base_url}?page={page_num}"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Turbo.az yeni struktur - products class-larÄ±
            # MÃ¼xtÉ™lif selector-lar sÄ±nayaq
            car_listings = (
                soup.find_all('div', class_='products-i') or
                soup.find_all('div', class_='product-item') or
                soup.find_all('a', class_='products-link') or
                soup.find_all('div', attrs={'data-id': True})
            )
            
            if not car_listings:
                # HTML-i faylda saxlayaq debugging Ã¼Ã§Ã¼n
                with open('turbo_debug.html', 'w', encoding='utf-8') as f:
                    f.write(str(soup.prettify()))
                print("âš ï¸ HTML strukturu faylda: turbo_debug.html")
                print("âš ï¸ MaÅŸÄ±n elanlarÄ± tapÄ±lmadÄ±. Alternativ scraping method istifadÉ™ edilir...")
                
                # Alternativ: bÃ¼tÃ¼n link-lÉ™ri tap
                all_links = soup.find_all('a', href=True)
                car_links = [link for link in all_links if '/autos/' in link.get('href', '')]
                
                if car_links:
                    print(f"âœ“ {len(car_links)} avtomobil linki tapÄ±ldÄ±")
                    for link in car_links[:50]:  # Ä°lk 50-ni gÃ¶tÃ¼r
                        car_data = self.extract_from_detail_page(link.get('href'))
                        if car_data:
                            self.cars_data.append(car_data)
                    return True if car_links else False
                
                return False
            
            for listing in car_listings:
                car_data = self.extract_car_data(listing)
                if car_data:
                    self.cars_data.append(car_data)
            
            print(f"SÉ™hifÉ™ {page_num}: {len(car_listings)} elan scrape edildi")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"SÉ™hifÉ™ {page_num} scrape xÉ™tasÄ±: {e}")
            return False

    def extract_from_detail_page(self, url):
        """ÆtraflÄ± sÉ™hifÉ™dÉ™n mÉ™lumat Ã§Ä±xarÄ±r"""
        try:
            if not url.startswith('http'):
                url = 'https://turbo.az' + url
            
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # SadÉ™lÉ™ÅŸdirilmiÅŸ extraction
            car = {}
            
            # BaÅŸlÄ±q
            title = soup.find('h1')
            if title:
                title_text = title.text.strip()
                parts = title_text.split(',')[0].split(' ')
                car['brand'] = parts[0] if len(parts) > 0 else None
                car['model'] = ' '.join(parts[1:]) if len(parts) > 1 else None
            
            # QiymÉ™t
            price = soup.find('div', class_='price')
            if price:
                car['price'] = self.clean_price(price.text)
            
            # ParametrlÉ™r
            params = soup.find_all('tr')
            for param in params:
                label = param.find('td', class_='label')
                value = param.find('td', class_='value')
                if label and value:
                    label_text = label.text.strip().lower()
                    value_text = value.text.strip()
                    
                    if 'buraxÄ±lÄ±ÅŸ' in label_text or 'il' in label_text:
                        car['year'] = self.clean_number(value_text)
                    elif 'yÃ¼rÃ¼ÅŸ' in label_text:
                        car['mileage'] = self.clean_number(value_text)
                    elif 'mÃ¼hÉ™rrik' in label_text and 'hÉ™cmi' in label_text:
                        car['engine_size'] = self.clean_number(value_text)
                    elif 'yanacaq' in label_text:
                        car['fuel_type'] = value_text.lower()
                    elif 'sÃ¼rÉ™tlÉ™r' in label_text:
                        car['transmission'] = value_text.lower()
            
            car['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            return car if car.get('price') else None
            
        except Exception as e:
            return None

    def extract_car_data(self, listing):
        """Bir elan mÉ™lumatlarÄ±nÄ± Ã§Ä±xarÄ±r"""
        try:
            car = {}
            
            # BaÅŸlÄ±q (Marka Model)
            title_elem = listing.find('div', class_='products-i__name')
            if title_elem:
                title = title_elem.text.strip()
                # "Mercedes-Benz E 200" -> brand: Mercedes-Benz, model: E 200
                parts = title.split(' ', 1)
                car['brand'] = parts[0] if len(parts) > 0 else None
                car['model'] = parts[1] if len(parts) > 1 else None
            else:
                return None
            
            # QiymÉ™t
            price_elem = listing.find('div', class_='product-price')
            if price_elem:
                car['price'] = self.clean_price(price_elem.text)
            else:
                return None  # QiymÉ™tsiz elanlarÄ± buraxÄ±rÄ±q
            
            # ParametrlÉ™r (Ä°l, YÃ¼rÃ¼ÅŸ, MÃ¼hÉ™rrik vÉ™ s.)
            params = listing.find_all('div', class_='products-i__attributes')
            if params:
                param_text = params[0].text if len(params) > 0 else ""
                
                # Ä°l
                year_match = re.search(r'(\d{4})', param_text)
                car['year'] = int(year_match.group(1)) if year_match else None
                
                # YÃ¼rÃ¼ÅŸ
                mileage_match = re.search(r'(\d+[\s\d]*)\s*km', param_text, re.IGNORECASE)
                car['mileage'] = self.clean_number(mileage_match.group(1)) if mileage_match else None
                
                # MÃ¼hÉ™rrik hÉ™cmi
                engine_match = re.search(r'(\d+\.?\d*)\s*L', param_text, re.IGNORECASE)
                car['engine_size'] = self.clean_number(engine_match.group(1)) if engine_match else None
            
            # ÅžÉ™hÉ™r
            city_elem = listing.find('div', class_='products-i__bottom-text')
            if city_elem:
                car['city'] = city_elem.text.strip()
            
            # Yanacaq nÃ¶vÃ¼, SÃ¼rÉ™tlÉ™r qutusu vÉ™ digÉ™r parametrlÉ™r
            # Bu mÉ™lumatlar É™traflÄ± sÉ™hifÉ™dÉ™ ola bilÉ™r, lakin sadÉ™lÉ™ÅŸdirmÉ™ Ã¼Ã§Ã¼n Ã¼mumi dÉ™yÉ™rlÉ™r veririk
            car['fuel_type'] = self.guess_fuel_type(listing.text)
            car['transmission'] = self.guess_transmission(listing.text)
            car['condition'] = 'yaxsi'  # Default
            car['owners'] = 1  # Default
            
            # YaradÄ±lma tarixi
            car['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            return car
            
        except Exception as e:
            print(f"MÉ™lumat Ã§Ä±xarma xÉ™tasÄ±: {e}")
            return None

    def guess_fuel_type(self, text):
        """MÉ™tndÉ™n yanacaq nÃ¶vÃ¼nÃ¼ tÉ™xmin edir"""
        text_lower = text.lower()
        if 'dizel' in text_lower or 'diesel' in text_lower:
            return 'dizel'
        elif 'hibrid' in text_lower or 'hybrid' in text_lower:
            return 'hibrid'
        elif 'elektrik' in text_lower or 'electric' in text_lower:
            return 'elektrik'
        elif 'qaz' in text_lower or 'gas' in text_lower:
            return 'qaz'
        else:
            return 'benzin'

    def guess_transmission(self, text):
        """MÉ™tndÉ™n sÃ¼rÉ™tlÉ™r qutusunu tÉ™xmin edir"""
        text_lower = text.lower()
        if 'mexaniki' in text_lower or 'manual' in text_lower:
            return 'mexaniki'
        elif 'robot' in text_lower:
            return 'robot'
        elif 'variator' in text_lower:
            return 'variator'
        else:
            return 'avtomat'

    def scrape_all(self, max_pages=100):
        """BÃ¼tÃ¼n sÉ™hifÉ™lÉ™ri scrape edir"""
        print(f"Turbo.az scraping baÅŸladÄ±...")
        print(f"Maksimum {max_pages} sÉ™hifÉ™ scrape edilÉ™cÉ™k")
        
        for page in range(1, max_pages + 1):
            success = self.scrape_page(page)
            
            if not success:
                print(f"SÉ™hifÉ™ {page}-dÉ™ problem. DayandÄ±rÄ±lÄ±r.")
                break
            
            # Rate limiting - saytÄ± yÃ¼klÉ™mÉ™mÉ™k Ã¼Ã§Ã¼n
            time.sleep(random.uniform(2, 4))
            
            # HÉ™r 10 sÉ™hifÉ™dÉ™ bir mÉ™lumat saxla
            if page % 10 == 0:
                self.save_data()
                print(f"âœ“ {len(self.cars_data)} mÉ™lumat indi saxlanÄ±ldÄ±")
        
        print(f"\nâœ“ Scraping tamamlandÄ±! CÉ™mi {len(self.cars_data)} avtomobil mÉ™lumatÄ± toplandÄ±")

    def save_data(self, filename='car_data.csv'):
        """MÉ™lumatlarÄ± CSV faylÄ±na yazÄ±r"""
        if not self.cars_data:
            print("SaxlanÄ±lacaq mÉ™lumat yoxdur")
            return
        
        df = pd.DataFrame(self.cars_data)
        
        # DublikatlarÄ± sil
        df = df.drop_duplicates(subset=['brand', 'model', 'year', 'price', 'mileage'])
        
        # CSV-yÉ™ yaz
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ“ MÉ™lumatlar '{filename}' faylÄ±na yazÄ±ldÄ± ({len(df)} sÉ™tir)")

    def clean_and_prepare_data(self, input_file='car_data.csv', output_file='car_data_cleaned.csv'):
        """MÉ™lumatlarÄ± tÉ™mizlÉ™yir vÉ™ ML Ã¼Ã§Ã¼n hazÄ±rlayÄ±r"""
        import os
        
        if not os.path.exists(input_file):
            print(f"âš ï¸ '{input_file}' faylÄ± tapÄ±lmadÄ±. Scraping uÄŸursuz oldu.")
            return None
        
        print("MÉ™lumatlar tÉ™mizlÉ™nir...")
        
        df = pd.read_csv(input_file)
        
        if len(df) == 0:
            print("âš ï¸ CSV boÅŸdur. MÉ™lumat yoxdur.")
            return None
        
        # Null dÉ™yÉ™rlÉ™ri sil
        df = df.dropna(subset=['price', 'year', 'mileage', 'brand', 'model'])
        
        # QiymÉ™t > 0
        df = df[df['price'] > 0]
        
        # Ä°l aralÄ±ÄŸÄ± (1990-2026)
        df = df[(df['year'] >= 1990) & (df['year'] <= 2026)]
        
        # YÃ¼rÃ¼ÅŸ > 0
        df = df[df['mileage'] >= 0]
        
        # Outlier-lÉ™ri sil (qiymÉ™t Ã§ox yÃ¼ksÉ™k/aÅŸaÄŸÄ±)
        if len(df) > 10:
            price_q1 = df['price'].quantile(0.01)
            price_q99 = df['price'].quantile(0.99)
            df = df[(df['price'] >= price_q1) & (df['price'] <= price_q99)]
        
        # TÉ™mizlÉ™nmiÅŸ datanÄ± saxla
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"âœ“ TÉ™mizlÉ™nmiÅŸ mÉ™lumatlar '{output_file}' faylÄ±na yazÄ±ldÄ± ({len(df)} sÉ™tir)")
        
        return df


def main():
    """Ana funksiya"""
    scraper = TurboAzScraper()
    
    print("\nâš ï¸ XÆBÆRDARLIQ: Turbo.az scraping Ã§É™tin ola bilÉ™r (CAPTCHA, anti-bot).")
    print("âš ï¸ Alternativ: HazÄ±r CSV faylÄ± istifadÉ™ edin vÉ™ ya manual mÉ™lumat toplayÄ±n.\n")
    
    # 1. Scraping et (10 sÉ™hifÉ™ test Ã¼Ã§Ã¼n)
    scraper.scrape_all(max_pages=10)
    
    # 2. ÆgÉ™r mÉ™lumat varsa saxla
    if scraper.cars_data:
        scraper.save_data('car_data_raw.csv')
        
        # 3. TÉ™mizlÉ™ vÉ™ hazÄ±rla
        scraper.clean_and_prepare_data('car_data_raw.csv', 'car_data_cleaned.csv')
    else:
        print("\nâŒ HeÃ§ bir mÉ™lumat scrape edilmÉ™di!")
        print("\nðŸ’¡ Alternativ hÉ™ll:")
        print("1. HazÄ±r CSV faylÄ± istifadÉ™ edin")
        print("2. VÉ™ ya É™llÉ™ mÉ™lumat toplayÄ±n")
        print("3. turbo_debug.html faylÄ±nÄ± yoxlayÄ±n vÉ™ selector-larÄ± dÃ¼zÉ™ldin")
        return
    
    print("\nðŸŽ‰ BÃ¼tÃ¼n É™mÉ™liyyatlar tamamlandÄ±!")
    print("car_data_cleaned.csv faylÄ± ML model Ã¼Ã§Ã¼n hazÄ±rdÄ±r")


if __name__ == "__main__":
    main()
